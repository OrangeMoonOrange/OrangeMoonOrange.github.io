---
layout:     post
title:      大数据集群搭建综合
subtitle:   如何搭建大数据集群
date:       2019-09-10
author:     XK
header-img: img/tag-bg-o.jpg
catalog: true
tags:
    - 大数据集群
---



# 1. 前置准备

**1）基础设施**

​	a.设置主机名

```shell
vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=node01
```
​	b.设置本机的ip到主机名的映射关系

```shell
vi /etc/hosts
xx.xx.xx.xx  node01
```
​	c.JDK的安装 ：并且设置系统环境变量

​	略

​	d.做时间同步

```shell
yum install ntp  -y
vi /etc/ntp.conf
	server ntp1.aliyun.com
service ntpd start
chkconfig ntpd on
```
​	e.免密登录：不管单机，还是集群，想要使用脚本登录，都需要免密登录

```shell
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa //秘钥生成
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys //配置自己免密

```

**tips**:如何免密登录其他节点？思路：如果B包含了A的公钥，A就可以免密的登陆

# 部署说明

> 集群的搭建核心就是角色的分配
>
> 每个集群的搭建，都需要经过：基础设施、集群配置、初始化启动、简单使用等步骤。

环境cetons7，虚拟机克隆的三台节点，依次为node01，node02，node03

本次使用的大数据组件版本是：cdh5.15.1。可以在在[CDH仓库](http://archive.cloudera.com/cdh5/cdh/5/)可以找到对应版本的各种分布式组件。

# 2. Zookeeper集群

> zk集群安装步骤

1.解压文件

解压文件到你的安装目录

2.配值zoo.cfg文件

进入你zk的解压目录，然后找到```conf```文件夹，使用```cp```将zoo_sample.cfg文件复制一份为zoo.cfg，然后```vi```编辑zoo.cfg文件，我们需要配置两个地方，```datadir```和节点的地址，其中datadir是zookeeper的存储数据的地方，节点地址说明整个集群地址、选举端口（3888）和通信端口（2888），例如我本机的配置如下：

```shell
datadir=/opt/zk
server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888
```

3.配置集群id 

在当前规划的节点中的数据目录```datadir```中，新建一个myid文件，各个机器中的myid要和步骤2中的数字一一对应。例如：在我node01机器上，我的myid 就应该是1，操作示例：

```shell
echo 1 >  /opt/zk/myid 
```

4.配置环境变量

我习惯配置在/etc/profile中，操作示例：

```shel
export ZOOKEEPER_HOME=/opt/zookeeper-3.4.6
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

5.分发到其他的节点

```shell
scp -r ./zookeeper-3.4.6  node02:`pwd`
scp -r ./zookeeper-3.4.6  node03:`pwd`
```

同样在其他节点需要配置myid和数据目录和环境变量。

6.启动zookeeper

分别在机器上运行```zkServer.sh start```命令，通过``jps``,查看java相关的进程，如果可以看到```QuorumPeerMain```进程，则说明zk启动成功，可以通过```zkServer.sh status```查看节点的状态。
 自此，恭喜！整个zk就搭建完毕。		

# 3. Hadoop集群

本节参考的地址

所有角色在哪里启动

主从集群：

主：单点，数据一致性好掌握

问题：（这是两个独立的问题）

- 单点故障
- 压力过大，内存受限

解决方案：

单点故障：

- HA
- 多个NN，主备切换

> 因为两个nn强一致性会破坏可用性，所以找一个折中的方案，使用集群的JN（类似ZK，选举，恢复，快速响应别人的操作（过半通过））：满足两个功能：1，解耦两个NN，不需要直接通信；1，持久化NN的数据，其他的NN会达到最终一致性。



过半：解决一致性的问题

压力多大，内存受限

- 联邦机制（元数据分片）【技术复杂】 
- 多个NN 

#### 伪分布式

比较简单：参考这篇文章：[Hadoop单节点安装](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)

#### HA模式

> 说明：本章开始搭建HA模式，其实在完全分布式中，NN节点存在两个问题：1）单点故障，2）压力过大，内存受限；其中单点故障问题可以通过Hadoop HA模式解决，问题二，需要通过联邦机制（元数据分片）来解决。本章只是解决Hadoop HA搭建的问题。且只是使用两个NN。

参考文章：

**1）文章参考**

[Hadoop HA搭建参考](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)

**2）整个集群的规划**

> 集群的搭建就是角色的规划，在集群搭建之前，建议先化成下面类似的表格

| HOST   | NN   | NN   | JNN  | DN   | ZKFC | ZK   |
| ------ | ---- | ---- | ---- | ---- | ---- | ---- |
| node01 | `    |      | `    |      | `    | `​    |
| node02 |      | `    | `    | `    | `    | `    |
| node03 |      |      | `    | `    | `    | `    |

**3）集群配置**

解压文件略过，假设你的解压目录在$Hadoop下，进入进入目录```$Hadoop/etc/hadoop```，可以看到一大堆配置文件，我们需要配置```core-site.xml```和```hdfs-site.xml```和```slaves```三个文件

a)core-site.xml文件

```xml
	core-site.xml
		<property>
		  <name>fs.defaultFS</name>
		  <value>hdfs://mycluster</value>//集群的命名空间，后面会用
		</property>
		 <property>
		   <name>ha.zookeeper.quorum</name>//zk集群的地址
		   <value>node01:2181,node02:2181,node03:2181</value>
		 </property>
```

b)hdfs-site.xml文件

```xml
#常规配置			
<property>
				<name>dfs.replication</name>//副本数量
				<value>2</value>
			 </property>
			 <property>
				<name>dfs.namenode.name.dir</name>//替换两个目录
				<value>/var/bigdata/hadoop/full/dfs/name</value>
			    </property>
			    <property>
				<name>dfs.datanode.data.dir</name>
				<value>/var/bigdata/hadoop/full/dfs/data</value>
			    </property>
#以下是  一对多，逻辑到物理节点的映射
		<property>
		  <name>dfs.nameservices</name>//命名空间
		  <value>mycluster</value>
		</property>
		<property>
		  <name>dfs.ha.namenodes.mycluster</name>//节点映射关系
		  <value>nn1,nn2</value>
		</property>
		<property>
		  <name>dfs.namenode.rpc-address.mycluster.nn1</name>//节点地址
		  <value>node01:8020</value>
		</property>
		<property>
		  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
		  <value>node02:8020</value>
		</property>
		<property>
		  <name>dfs.namenode.http-address.mycluster.nn1</name>
		  <value>node01:50070</value>
		</property>
		<property>
		  <name>dfs.namenode.http-address.mycluster.nn2</name>
		  <value>node02:50070</value>
		</property>

		#以下是JN在哪里启动，数据存那个磁盘  mycluster区分不同的环境
		<property>
		  <name>dfs.namenode.shared.edits.dir</name>
		  <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
		</property>
		<property>
		  <name>dfs.journalnode.edits.dir</name>
		  <value>/var/hadoop/ha/dfs/jn</value>
		</property>
		
		#HA角色切换的代理类和实现方法，我们用的ssh免密
		<property>
		  <name>dfs.client.failover.proxy.provider.mycluster</name>		     <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		</property>
		<property>
		  <name>dfs.ha.fencing.methods</name>
		  <value>sshfence</value>
		</property>
		<property>//秘钥在哪里 这里使用的root用户
		  <name>dfs.ha.fencing.ssh.private-key-files</name>
		  <value>/root/.ssh/id_dsa</value>
		</property>
		
		#开启自动化： 启动zkfc
		 <property>
		   <name>dfs.ha.automatic-failover.enabled</name>
		   <value>true</value>
		 </property>




```

c)slaves文件

使用 vi slaves文件,添加从节点

node02
​node03 

**4）分发到其他节点**

```shell
scp -r ./hadoop/  node02:`pwd`
scp -r ./hadoop/  node03:`pwd`
```

**5）格式化和启动**

需要依次按照下面的步骤（严格按照）来做：

 -  先启动JN   hadoop-daemon.sh start journalnode 
 -  选择一个NN 做格式化：hdfs namenode -format   <只有第一次搭建做，以后不用做
 -  启动这个格式化的NN ，以备另外一台同步  hadoop-daemon.sh start namenode 
 -  在另外一台机器中： hdfs namenode -bootstrapStandby
 -  格式化zk：   hdfs zkfc  -formatZK     <只有第一次搭建做，以后不用做
 -    start-dfs.sh	

**6）界面**

访问WEBUI地址：node01:50070



# 4. Spark集群

#### standalone模式

搭建参考：[spark standalone模式搭建流程](http://spark.apache.org/docs/2.4.2/spark-standalone.html)

**1）基础设施**

在每个节点配置jdk，部署hdfs，免密登录等

**2）spark的安装**

下载对应的版本的spark，必须和你的hadoop版本一一对应，或者你可以自己选择从源码开始编译spark。

**3）部署细节**

首先是节点规划，node01作为mater，其他两个节点作为worker。

选择在node01上，解压spark 安装包，然后在配置spark_home的环境变量，然后进入$SPARK_HOME/conf目录，执行cp ``spark-env.sh.template spark-env.sh``,同时也执行``cp slaves.template slaves``，在``slaves``中，配置从节点的主机端口，在``spark-env.sh``中配置下面这些配置，在配置文件中每项的意思都有详细的解释。大致就是配置整个分布式集群中，mater和work之间通信的端口，和work节点的资源配置，因为本次spark的底层数据存储是hdfs，所以还需要配置hadoop_conf_dir告诉spark集群如何找到hadoop。

```shell
export HADOOP_CONF_DIR=
exort SPARK_MASTER_HOST=
export SPARK_MASTER_WEBUI_PORT=
export SPARK_WORKER_CORES=
export SPARK_WORKER_MEMORY=
```

最后，需要在$SPARK_HOME/sbin目录中，找到```spark-config.sh``这个脚本，在里面追加上Java的目录。也就是在文件的最后加上：export JAVA_HOME=

4）分发

将node01上的spark包分发到其他work节点。

5）简单使用

在$SPARK_HOME/sbin目录下，使用命令``./spark-shell --master spark://node01:7077``，启动spark-shell,做一个简单的wordcount，首先准备单词文件data.txt，然后上传到hdfs的根目录。

```scala
//词频统计
sc.textFile("/data.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect.foreach(println)
```

5）总结

多使用 --help，会看到很多帮助文档

# Hbase

# Yarn

# Kafka

# Hive

